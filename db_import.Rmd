---
title: "Data import to insect_monitoring database 2020"
author: "Jens Å"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  NinaR::jensAnalysis:
    highlight: tango
    fig_caption: yes
    toc: yes
---


**Todo**

* Fix species names
 - get rid of fa_, pa_ etc,
 - split norwegian and latin into separate columns
 - fix my manually entered names into correct latin names



* Add main data

 
```{r, include = F}
#Some common packages, loading rmarkdown doesn't like the messages from tidyverse, so we don't include this in the document'
require(tidyverse)
require(DBI)
require(RPostgres)
#require(RPostgreSQL)
require(ggplot2)
require(xtable)
require(NinaR)
require(sf)
require(openxlsx)
require(dbplyr)
require(hms)
```


```{r setup, include=FALSE}
#This is optional
#I choose the 'styler' package for tidying the code to preserve indentations
#I set the cutoff for code tidying to 60, but this doesn't currently work with styler.
#Set tidy = True to get the knitr default
#I want all figures as png and pdf in high quality in a subfolder called figure 

knitr::opts_chunk$set(echo = TRUE, 
                      tidy = "styler",
                      dev = c("png", "pdf"),
                      dpi = 600,
                      fig.path = "figure/",
                      tidy.opts = list(width.cutoff = 60)
                      )

options(xtable.comment = F, 
        xtable.include.rownames = F, 
        nina.logo.y.pos = 0.15)
palette(ninaPalette())
```



```{r, include = F, eval = T}
source("~/.rpgpass")
#This connects to the gisdatabase with a DBI connection named `con`.
#Use for example dbGetQuery(con, "SELECT * FROM ....") to query the database
dbDisconnect(con)
con <- dbConnect(Postgres(), 
                 dbname = "insect_monitoring",
                 host = "ninradardata01.nina.no",
                 user = username,
                 password = password
                 )

rm(username, password)
```

Intro
=========
I'll try to collect most of the data import to the database in 2020 here.


Trap locations
=========
```{r}
trap_loc <- read_delim("../../Data/trap_locations/all_trap_locations_2020.csv",
                     delim = ";")

trap_loc <- trap_loc %>% 
  mutate(year = 2020,
         elev_data = ifelse(is.na(elev), FALSE, TRUE)) %>% 
  mutate(elev = ifelse(is.na(elev), 0, elev)) %>% 
           st_as_sf(coords = c("lon", "lat", "elev"),
                    crs = 25833) %>% 
  mutate(geom = geometry) %>% 
  st_sf(sf_column_name = "geom") %>% 
  select(-geometry)


```
```{r}

dbSendQuery(con, "DROP TABLE IF EXISTS locations.trap_locations;")

create_trap_loc_q <- "
CREATE TABLE locations.trap_locations (
id serial PRIMARY KEY,
location text NOT NULL,
year integer NOT NULL,
trap text NOT NULL,
gps_type text NOT NULL,
elev_data BOOLEAN NOT NULL,
geom Geometry(Pointz, 25833)
)

"

dbSendQuery(con, create_trap_loc_q)

```

```{r}

dbSendQuery(con, "CREATE INDEX ON locations.trap_locations USING GIST(geom);")
```


```{r, eval = F}
dbWriteTable(con, 
             Id(schema = "traps", table = "trap_locations"),
             trap_loc)
```


Quick plot of the traps
========
Here we make a simple plot of just one location.

```{r}
traps_2020 <- read_sf(con, c("traps", "trap_locations"))

```

```{r skog_2_traps}
traps_2020 %>% 
  filter(location == "Skog_4") %>% 
ggplot(.) +
  geom_sf(aes(color = gps_type)) + 
  scale_color_nina(name = "GPS-type") +
 geom_sf_label(aes(label = trap),
               nudge_x = 2,
               nudge_y = 2) +
  coord_sf(datum = sf::st_crs(25833)) +
  labs(caption = "\U00a9 OpenStreetMap contributors")
```

Locations 2020
========
```{r}
loc <- read_delim("../../Data/trap_locations/locations_2020.csv",
                     delim = ",")

loc <- loc %>% 
  mutate(year = 2020) %>% 
  select(location = locality,
         habitat_type,
         year,
         ano_flate_id, 
         ssbid)

```
```{r}

dbSendQuery(con, "DROP TABLE IF EXISTS locations.locations;")

create_loc_q <- "
CREATE TABLE locations.locations (
id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
location text NOT NULL,
habitat_type text NOT NULL,
year integer NOT NULL,
ano_flate_id text,
ssbid bigint NOT NULL
)

"

dbSendQuery(con, create_loc_q)

```

```{r}

dbSendQuery(con, "CREATE INDEX ON locations.locations USING BTREE(location);")
dbSendQuery(con, "CREATE INDEX ON locations.locations USING BTREE(ano_flate_id);")
dbSendQuery(con, "CREATE INDEX ON locations.locations USING BTREE(habitat_type);")
dbSendQuery(con, "CREATE INDEX ON locations.locations USING BTREE(ssbid);")
dbSendQuery(con, "CREATE INDEX ON locations.locations USING BTREE(year);")
```

```{r, eval = F}
dbWriteTable(con, 
             Id(schema = "locations", table = "locations"),
             loc,
             append = T)
```



Import ANO records
========
We got an export from the ANO-database of the Environmental agency, of the registered data for our survey locations in 2020. These are the 10 forest sites, that were surveyed by a separate project. I've punched the data we collected for the Semi-natural grasslands in the same format. These sites only contain records from one of the 18 ANO circles in the location though.

Here I will create database tables to suit the data and import the records.



```{r}
md_ano_points <- read.xlsx("../../Data/ANO_eksport_MD/ANO_ekspot_excel.xlsx", 
                           detectDates = T,
                           sheet = 1) %>% 
  mutate(Registeringsdato = convertToDate(Registeringsdato, origin = "1899-12-30"),
         CreationDate = convertToDateTime(CreationDate, origin = "1899-12-30 00:00"),
         EditDate = convertToDateTime(EditDate, origin = "1899-12-30 00:00")) %>% 
  as_tibble()
```

Check data format

```{r}
md_ano_points %>% 
  print(n = 1,
        width = Inf)

```

We need to change some column names to fit PostgreSQL. No capital letters and dots.

```{r}
old_names <- names(md_ano_points)
new_names <- tolower(old_names)
new_names <- gsub("\\.", "_", new_names)
new_names <- gsub(":", "", new_names)
new_names <- gsub("\\?", "", new_names)
new_names <- gsub("\\(", "", new_names)
new_names <- gsub("\\)", "", new_names)
new_names <- gsub("\\%", "prc", new_names)
new_names <- gsub("\\-", "_", new_names)
new_names <- gsub("\\/", "_", new_names)
new_names[new_names=="tilstedeværelse_av_naturtype_etter_miljødirektoratets_instruks"] <- "tilstedeværelse_av_naturtype_etter_miljødirektoratets_instruk"
new_names[new_names=="globalid"] <- "global_id"
new_names[new_names=="editdate"] <- "edit_date"
new_names[new_names=="creationdate"] <- "creation_date"

new_names
```
```{r}
names(md_ano_points) <- new_names
```

There is an error in the ano_flate_id for the locations with four digit numbers. They are called AN01234 instead of ANO1234. 

```{r}
md_ano_points <- md_ano_points %>% 
  mutate(ano_flate_id = gsub("(AN0)(.*)", "ANO\\2", ano_flate_id)) 

```

Lengdegrad and breddegrad contains the same info, should be lat, lon. Also, we don't need the row numbers or the column with instructions for the right hand columns. Lastly, we don't need the "Arter registrert" because this will be added in the herb_species table later.

```{r}
md_ano_points <- md_ano_points %>% 
  select(-c(her_skal_dekning_av_ano_variabler_på_250_m2_registeres__visuell_estimering_,
            objectid,
            breddegrad,
            lengdegrad,
            arter_registrert)) %>% 
  mutate(x = ifelse(x == 0, NA, x),
         y = ifelse(y == 0, NA, y))
```

```{r, eval = F}
dbSendQuery(con, "CREATE SCHEMA IF NOT EXISTS ano;")
"

dbSendStatement(con, "
GRANT ALL ON SCHEMA ano TO ag_pgsql_radardata_admin;")

dbSendStatement(con, "GRANT USAGE ON SCHEMA ano TO ag_pgsql_radardata_ro;")

dbSendStatement(con, "GRANT USAGE ON SCHEMA ano TO ag_pgsql_radardata_rw;")

dbSendStatement(con, "ALTER DEFAULT PRIVILEGES IN SCHEMA ano
GRANT ALL ON TABLES TO ag_pgsql_radardata_admin;")

dbSendStatement(con, "ALTER DEFAULT PRIVILEGES IN SCHEMA ano
GRANT SELECT ON TABLES TO ag_pgsql_radardata_ro;")

dbSendStatement(con, "ALTER DEFAULT PRIVILEGES IN SCHEMA ano
GRANT INSERT, SELECT, UPDATE ON TABLES TO ag_pgsql_radardata_rw;")

```

```{r, eval = F}
dbSendStatement(con, "DROP TABLE IF EXISTS ano.survey_points CASCADE;")

create_ano_point_Q <- "
CREATE TABLE ano.survey_points (
global_id UUID PRIMARY KEY,
registeringsdato date,
klokkeslett_start time(0),
værforhold text,
ano_flate_id text,
ano_punkt_id int,
hovedøkosystem_sirkel_250m2 text,
andel_prc_av_sirkel_250m2_i_hovedøkosystemet double precision,
er_punktet_utilgjengelig text,
begrunnelse text,
hvilken_type_gps_brukes text,
nøyaktighet text,
kartleggingsenhet_15000_senterpunkt text,
hovedtype_1m2 text,
ke_beskrivelse_1m2 text,
kartleggingsenhet_15000_i_250m2_sirkel_den_vanligste text,
hovedtype_250m2 text,
ke_beskrivelse_250m2 text,
andel_prc_av_sirkel_250m2_med_denne_kartleggingsenheten double precision,
grøftingsintensitet_7gr_gi text,
aktuell_bruksintensitet_7jb_ba text,
beitetrykk_7jb_bt text,
slåtteintensitet_7jb_si text,
spor_etter_ferdsel_med_tunge_kjøretøy_7tk text,
spor_etter_slitasje_og_slitasjebetinget_erosjon_7se text,
tilstedeværelse_av_naturtype_etter_miljødirektoratets_instruk text,
velg_naturtype text,
kommentar_knyttet_til_naturtyperegistreringene text,
total_dekning_prc_av_karplanter_registert double precision,
dekning_prc_av_karplanter_i_feltsjikt double precision,
dekning_prc_av_moser double precision,
dekning_prc_av_torvmoser double precision,
dekning_prc_av_lav double precision,
dekning_prc_av_strø double precision,
dekning_prc_av_bar_jord_grus_stein_berg double precision,
dekning_prc_av_alger_crust double precision,
kommentar_knyttet_til_registreringene_i_ruten_1m2 text,
er_det_satt_ned_fastmerker text,
kommentar_om_merkingen_av_ruten text,
samlet_dekning_prc_av_krypende_vier double precision,
samlet_dekning_prc_av_ikke_krypende_vier double precision,
total_dekning_prc_av_vedplanter_i_feltsjikt double precision,
dekning_prc_av_busker_i_busksjikt double precision,
dekning_prc_av_tresjikt double precision,
dekning_prc_av_død_skadet_røsslyng double precision,
med_eller_uten_røsslyngblad text,
dekning_prc_av_problemarter double precision,
total_dekning_prc_av_fremmede_arter double precision,
kommentar_knyttet_til_registreringene_på_250_m2_sirkel text,
klokkeslett_slutt time(0),
creation_date timestamp,
creator text,
edit_date timestamp,
editor text,
x double precision,
y double precision,
geom_punched Geometry(Point, 25833))
;
"

dbSendStatement(con, create_ano_point_Q)

dbSendStatement(con, "CREATE INDEX ON ano.survey_points USING BTREE(global_id);")
dbSendStatement(con, "CREATE INDEX ON ano.survey_points USING BTREE(ano_flate_id);")
dbSendStatement(con, "CREATE INDEX ON ano.survey_points USING BTREE(ano_punkt_id);")
dbSendStatement(con, "CREATE INDEX ON ano.survey_points USING GIST(geom_punched);")



geom_trg_fn <- "
CREATE OR REPLACE FUNCTION fn_create_geom_from_xy() RETURNS trigger AS $fn_create_geom_from_xy$
  BEGIN  
	NEW.geom_punched = ST_Transform(ST_SetSRID(ST_MakePoint(NEW.x, NEW.y), 4326), 25833);
  RETURN NEW;
  END;
$fn_create_geom_from_xy$ LANGUAGE plpgsql;
"

geom_tr <- "
CREATE TRIGGER tr_test_table_inserted
  BEFORE INSERT OR UPDATE ON ano.survey_points
  FOR EACH ROW
  EXECUTE PROCEDURE fn_create_geom_from_xy();"

dbSendStatement(con, geom_trg_fn)

dbSendStatement(con, "DROP TRIGGER IF EXISTS tr_test_table_inserted ON ano.survey_points;")
dbSendStatement(con, geom_tr)



```


```{r, eval = F}
dbSendStatement(con, "DELETE FROM ano.survey_points;")

dbWriteTable(con,
             Id(schema = "ano", table = "survey_points"),
             md_ano_points,
             append = T)
```


Create reference table of 18 ANO points in all potential ANO squares
===================
Quite alot of the ANO-data is missing coordinates. Assuming they are located at the theoretical locations of the ANO survey plan, we can infer the positions from there. We need to add a reference table of all potential ANO-squares, and compute the 18 survey points within each location.

Read in all 10000 potential ANO sites
---------

```{r}
ano_10000 <- read_sf("../../Data/AKO_sample/AKO-sample-10000-pol.shp",
                     crs = 25833)

ano_10000 <- ano_10000 %>% 
  mutate(anoid = gsub("(AKO)(.*)", "ANO\\2", akoid)) %>% 
  select(ssbid,
         anonum = akonum,
         anoid) %>% 
    rename(geom = geometry)

ano_10000
```

```{r}
dbSendStatement(con, "DROP TABLE IF EXISTS ano.ano_10000;")

create_ano_q <- "
CREATE TABLE ano.ano_10000 (
ssbid bigint PRIMARY KEY,
anonum int,
anoid text,
geom Geometry(Polygon, 25833)
)
"
dbSendStatement(con, create_ano_q)

st_write(dsn = con,
         layer = Id(schema = "ano", table = "ano_10000"),
         obj = ano_10000,
         append = T)

```

```{r}
dbSendStatement(con, "CREATE INDEX ON ano.ano_10000 USING Gist(geom);")
dbSendStatement(con, "CREATE INDEX ON ano.ano_10000 USING BTREE(anonum);")
dbSendStatement(con, "CREATE INDEX ON ano.ano_10000 USING BTREE(anoid);")

```

Add table with ANO point offsets. This has a logic to it, got the recipe from Vegard B I think.

```{r}
ano_offsets <- read_csv("../../GIS/Feltlok_2020/ANO/AKO_punkt_offset/ako_point_offset.csv") %>% 
  as_tibble()
ano_offsets
```
```{r}
create_ano_offset <- "
CREATE TABLE ano.point_offsets (
id UUID PRIMARY KEY,
x int,
y int,
ano_point_name int);
"

dbSendStatement(con,
                create_ano_offset)

```

```{r}
dbSendStatement(con, "DELETE FROM ano.point_offsets;")

dbWriteTable(con,
             Id(schema = "ano", table = "point_offsets"),
             ano_offsets,
             append = T)
```

```{r, eval = F}
dbSendStatement(con, "DROP TABLE IF EXISTS ano.ano_18_points;")

create_ano_points <- "
CREATE TABLE ano.ano_18_points as (
SELECT gen_random_uuid() as id, anoid as ano_id, ano_point_name as ano_point_id, st_setsrid(st_makepoint(offset_x, offset_y), 25833)::Geometry(Point, 25833) as geom
FROM 
(SELECT startpoint.x + o.x as offset_x, startpoint.y + o.y as offset_y, anoid, ano_point_name
FROM  ano.point_offsets o,
(SELECT ssbid, anoid, min(x) x, min(y) y
FROM (
SELECT ssbid, anoid, ST_x((st_dumppoints(ST_Extent(s.geom))).geom) as x, 
ST_Y((st_dumppoints(ST_Extent(s.geom))).geom) as y
FROM ano.ano_10000 s
GROUP BY ssbid
 ) foo
 GROUP BY foo.ssbid, foo.anoid) as startpoint) points
) 
"

dbSendStatement(con,
                create_ano_points)

```
```{r}
dbSendStatement(con, "CREATE INDEX ON ano.ano_18_points USING BTREE(ano_id);")
dbSendStatement(con, "CREATE INDEX ON ano.ano_18_points USING BTREE(ano_point_id);")

```

Add inferred geometry to ANO-table.

```{r}
add_ano_point_geom <- "
ALTER TABLE ano.survey_points 
ADD COLUMN geom_inferred Geometry(Point, 25833);
"
dbSendStatement(con, add_ano_point_geom)

```

```{r}
add_inf_geoms <- "
UPDATE ano.survey_points s
SET geom_inferred = p.geom
FROM ano.ano_18_points p
WHERE s.ano_flate_id = p.ano_id
AND s.ano_punkt_id = p.ano_point_id
"

dbSendStatement(con, add_inf_geoms)

```

Add the ANO registered plant species
---------


```{r}
md_ano_species <- read.xlsx("../../Data/ANO_eksport_MD/ANO_ekspot_excel.xlsx", 
                           detectDates = T,
                           sheet = 2) %>% 
  mutate(CreationDate = convertToDateTime(CreationDate, origin = "1899-12-30 00:00"),
         EditDate = convertToDateTime(EditDate, origin = "1899-12-30 00:00")) %>% 
  as_tibble()
```

Fix some column names
```{r}
old_names <- names(md_ano_species)
new_names <- tolower(old_names)
new_names <- gsub("\\.", "_", new_names)
new_names <- gsub(":", "", new_names)
new_names <- gsub("\\?", "", new_names)
new_names <- gsub("\\(", "", new_names)
new_names <- gsub("\\)", "", new_names)
new_names <- gsub("\\%", "prc", new_names)
new_names <- gsub("\\-", "_", new_names)
new_names <- gsub("\\/", "_", new_names)
new_names[new_names=="globalid"] <- "global_id"
new_names[new_names=="editdate"] <- "edit_date"
new_names[new_names=="creationdate"] <- "creation_date"
new_names[new_names=="parentglobalid"] <- "parent_global_id"
new_names
```

```{r}
names(md_ano_species) <- new_names
```

Select just the columns to be imported.

```{r}

md_ano_species <- md_ano_species %>% 
  select(global_id,
         parent_global_id,
         navn,
         dekning_prc,
         creation_date,
         creator,
         edit_date,
         editor)
```


```{r, eval = F}
dbSendStatement(con, "DROP TABLE IF EXISTS ano.herb_species;")

create_ano_species_Q <- "
CREATE TABLE ano.herb_species (
global_id UUID PRIMARY KEY,
parent_global_id UUID,
navn text,
dekning_prc double precision,
creation_date timestamp,
creator text,
edit_date timestamp,
editor text,
CONSTRAINT fk_parent FOREIGN KEY(parent_global_id)
REFERENCES ano.survey_points(global_id)
ON DELETE RESTRICT
ON UPDATE CASCADE
);"



dbSendStatement(con, create_ano_species_Q)

dbSendStatement(con, "CREATE INDEX ON ano.herb_species USING BTREE(global_id);")
dbSendStatement(con, "CREATE INDEX ON ano.herb_species USING BTREE(parent_global_id);")
dbSendStatement(con, "CREATE INDEX ON ano.herb_species USING BTREE(navn);")
```
```{r}
dbWriteTable(con,
             Id(schema = "ano", table = "herb_species"),
             md_ano_species,
             append = T)
```

Add tree species
-------------

```{r}
md_ano_trees <- read.xlsx("../../Data/ANO_eksport_MD/ANO_ekspot_excel.xlsx", 
                           detectDates = T,
                           sheet = 3) %>% 
  mutate(CreationDate = convertToDateTime(CreationDate, origin = "1899-12-30 00:00"),
         EditDate = convertToDateTime(EditDate, origin = "1899-12-30 00:00")) %>% 
  as_tibble()
```

Fix some column names
```{r}
old_names <- names(md_ano_trees)
new_names <- tolower(old_names)
new_names <- gsub("\\.", "_", new_names)
new_names <- gsub(":", "", new_names)
new_names <- gsub("\\?", "", new_names)
new_names <- gsub("\\(", "", new_names)
new_names <- gsub("\\)", "", new_names)
new_names <- gsub("\\%", "prc", new_names)
new_names <- gsub("\\-", "_", new_names)
new_names <- gsub("\\/", "_", new_names)
new_names[new_names=="globalid"] <- "global_id"
new_names[new_names=="editdate"] <- "edit_date"
new_names[new_names=="creationdate"] <- "creation_date"
new_names[new_names=="parentglobalid"] <- "parent_global_id"
new_names
```

```{r}
names(md_ano_trees) <- new_names
```

Select just the columns to be imported.

```{r}

md_ano_trees <- md_ano_trees %>% 
  select(global_id,
         parent_global_id,
         navn,
         creation_date,
         creator,
         edit_date,
         editor)
```


```{r, eval = F}
dbSendStatement(con, "DROP TABLE IF EXISTS ano.tree_species;")

create_ano_trees_Q <- "
CREATE TABLE ano.tree_species(
global_id UUID PRIMARY KEY,
parent_global_id UUID,
navn text,
creation_date timestamp,
creator text,
edit_date timestamp,
editor text,
CONSTRAINT fk_parent FOREIGN KEY(parent_global_id)
REFERENCES ano.survey_points(global_id)
ON DELETE RESTRICT
ON UPDATE CASCADE
);"



dbSendStatement(con, create_ano_trees_Q)

dbSendStatement(con, "CREATE INDEX ON ano.tree_species USING BTREE(global_id);")
dbSendStatement(con, "CREATE INDEX ON ano.tree_species USING BTREE(parent_global_id);")
dbSendStatement(con, "CREATE INDEX ON ano.tree_species USING BTREE(navn);")
```
```{r}
dbWriteTable(con,
             Id(schema = "ano", table = "tree_species"),
             md_ano_trees,
             append = T)
```

Add problem species
-----------------
```{r}
md_ano_problem <- read.xlsx("../../Data/ANO_eksport_MD/ANO_ekspot_excel.xlsx", 
                           detectDates = T,
                           sheet = 4) %>% 
  mutate(CreationDate = convertToDateTime(CreationDate, origin = "1899-12-30 00:00"),
         EditDate = convertToDateTime(EditDate, origin = "1899-12-30 00:00")) %>% 
  as_tibble()
```

Fix some column names
```{r}
old_names <- names(md_ano_problem)
new_names <- tolower(old_names)
new_names <- gsub("\\.", "_", new_names)
new_names <- gsub(":", "", new_names)
new_names <- gsub("\\?", "", new_names)
new_names <- gsub("\\(", "", new_names)
new_names <- gsub("\\)", "", new_names)
new_names <- gsub("\\%", "prc", new_names)
new_names <- gsub("\\-", "_", new_names)
new_names <- gsub("\\/", "_", new_names)
new_names[new_names=="globalid"] <- "global_id"
new_names[new_names=="editdate"] <- "edit_date"
new_names[new_names=="creationdate"] <- "creation_date"
new_names[new_names=="parentglobalid"] <- "parent_global_id"
new_names
```

```{r}
names(md_ano_problem) <- new_names
```

Select just the columns to be imported.

```{r}

md_ano_problem <- md_ano_problem %>% 
  select(global_id,
         parent_global_id,
         navn,
         creation_date,
         creator,
         edit_date,
         editor)
```


```{r, eval = F}
dbSendStatement(con, "DROP TABLE IF EXISTS ano.problem_species;")

create_ano_problem_Q <- "
CREATE TABLE ano.problem_species(
global_id UUID PRIMARY KEY,
parent_global_id UUID,
navn text,
creation_date timestamp,
creator text,
edit_date timestamp,
editor text,
CONSTRAINT fk_parent FOREIGN KEY(parent_global_id)
REFERENCES ano.survey_points(global_id)
ON DELETE RESTRICT
ON UPDATE CASCADE
);"



dbSendStatement(con, create_ano_problem_Q)

dbSendStatement(con, "CREATE INDEX ON ano.problem_species USING BTREE(global_id);")
dbSendStatement(con, "CREATE INDEX ON ano.problem_species USING BTREE(parent_global_id);")
dbSendStatement(con, "CREATE INDEX ON ano.problem_species USING BTREE(navn);")
```
```{r}
dbWriteTable(con,
             Id(schema = "ano", table = "problem_species"),
             md_ano_problem,
             append = T)
```

Add alien species
------------
```{r}
md_ano_alien <- read.xlsx("../../Data/ANO_eksport_MD/ANO_ekspot_excel.xlsx", 
                           detectDates = T,
                           sheet = 5) %>% 
  mutate(CreationDate = convertToDateTime(CreationDate, origin = "1899-12-30 00:00"),
         EditDate = convertToDateTime(EditDate, origin = "1899-12-30 00:00")) %>% 
  as_tibble()
```

Fix some column names
```{r}
old_names <- names(md_ano_alien)
new_names <- tolower(old_names)
new_names <- gsub("\\.", "_", new_names)
new_names <- gsub(":", "", new_names)
new_names <- gsub("\\?", "", new_names)
new_names <- gsub("\\(", "", new_names)
new_names <- gsub("\\)", "", new_names)
new_names <- gsub("\\%", "prc", new_names)
new_names <- gsub("\\-", "_", new_names)
new_names <- gsub("\\/", "_", new_names)
new_names[new_names=="globalid"] <- "global_id"
new_names[new_names=="editdate"] <- "edit_date"
new_names[new_names=="creationdate"] <- "creation_date"
new_names[new_names=="parentglobalid"] <- "parent_global_id"
new_names
```

```{r}
names(md_ano_alien) <- new_names
```

Select just the columns to be imported.

```{r}

md_ano_alien <- md_ano_alien %>% 
  select(global_id,
         parent_global_id,
         navn,
         creation_date,
         creator,
         edit_date,
         editor)
```


```{r, eval = F}
dbSendStatement(con, "DROP TABLE IF EXISTS ano.alien_species;")

create_ano_alien_Q <- "
CREATE TABLE ano.alien_species(
global_id UUID PRIMARY KEY,
parent_global_id UUID,
navn text,
creation_date timestamp,
creator text,
edit_date timestamp,
editor text,
CONSTRAINT fk_parent FOREIGN KEY(parent_global_id)
REFERENCES ano.survey_points(global_id)
ON DELETE RESTRICT
ON UPDATE CASCADE
);"



dbSendStatement(con, create_ano_alien_Q)

dbSendStatement(con, "CREATE INDEX ON ano.alien_species USING BTREE(global_id);")
dbSendStatement(con, "CREATE INDEX ON ano.alien_species USING BTREE(parent_global_id);")
dbSendStatement(con, "CREATE INDEX ON ano.alien_species USING BTREE(navn);")
```
```{r}
dbWriteTable(con,
             Id(schema = "ano", table = "alien_species"),
             md_ano_alien,
             append = T)
```




Add our own ANO-data for the semi-natural locations
============
I used the MD-data as a punching template, so we need to fix these similarly.

Points first
------------
```{r}
nina_ano_points <- read.xlsx("../../Data/ANO_reg/ANO_punching_Jens.xlsx", 
                           detectDates = T,
                           sheet = 1) %>%
  as_tibble()
```

Check data format

```{r}
nina_ano_points %>% 
  print(n = 1,
        width = Inf)

```



We need to change some column names to fit PostgreSQL. No capital letters and dots.

```{r}
old_names <- names(nina_ano_points)
new_names <- tolower(old_names)
new_names <- gsub("\\.", "_", new_names)
new_names <- gsub(":", "", new_names)
new_names <- gsub("\\?", "", new_names)
new_names <- gsub("\\(", "", new_names)
new_names <- gsub("\\)", "", new_names)
new_names <- gsub("\\%", "prc", new_names)
new_names <- gsub("\\-", "_", new_names)
new_names <- gsub("\\/", "_", new_names)
new_names[new_names=="tilstedeværelse_av_naturtype_etter_miljødirektoratets_instruks"] <- "tilstedeværelse_av_naturtype_etter_miljødirektoratets_instruk"
new_names[new_names=="globalid"] <- "global_id"
new_names[new_names=="editdate"] <- "edit_date"
new_names[new_names=="creationdate"] <- "creation_date"

new_names
```
```{r}
names(nina_ano_points) <- new_names
```


```{r}
nina_ano_points <- nina_ano_points %>% 
    mutate(x = lengdegrad,
         y = breddegrad) %>% 
  select(-c(her_skal_dekning_av_ano_variabler_på_250_m2_registeres__visuell_estimering_,
            objectid,
            arter_registrert,
            lengdegrad,
            breddegrad))

```

```{r}
dbSendStatement(con, "DELETE FROM ano.survey_points WHERE creator = 'Jens_Astrom_manual';")

dbWriteTable(con,
             Id(schema = "ano", table = "survey_points"),
             nina_ano_points,
             append = T)
```

Add inferred points
```{r}
add_inf_geoms <- "
UPDATE ano.survey_points s
SET geom_inferred = p.geom
FROM ano.ano_18_points p
WHERE s.ano_flate_id = p.ano_id
AND s.ano_punkt_id = p.ano_point_id
"

dbSendStatement(con, add_inf_geoms)

```

```{r}
add_inf_geoms <- "
UPDATE ano.survey_points s
SET geom_inferred = p.geom
FROM ano.ano_18_points p
WHERE s.ano_flate_id = p.ano_id
AND s.ano_punkt_id = p.ano_point_id
"

dbSendStatement(con, add_inf_geoms)

```

Herb species
------

```{r}
nina_ano_species <- read.xlsx("../../Data/ANO_reg/ANO_punching_Jens.xlsx", 
                           detectDates = T,
                           sheet = 2) %>% 
  as_tibble()
```

Fix some column names
```{r}
old_names <- names(nina_ano_species)
new_names <- tolower(old_names)
new_names <- gsub("\\.", "_", new_names)
new_names <- gsub(":", "", new_names)
new_names <- gsub("\\?", "", new_names)
new_names <- gsub("\\(", "", new_names)
new_names <- gsub("\\)", "", new_names)
new_names <- gsub("\\%", "prc", new_names)
new_names <- gsub("\\-", "_", new_names)
new_names <- gsub("\\/", "_", new_names)
new_names[new_names=="globalid"] <- "global_id"
new_names[new_names=="editdate"] <- "edit_date"
new_names[new_names=="creationdate"] <- "creation_date"
new_names[new_names=="parentglobalid"] <- "parent_global_id"
new_names
```

```{r}
names(nina_ano_species) <- new_names
```

Select just the columns to be imported.

```{r}

nina_ano_species <- nina_ano_species %>% 
  select(global_id,
         parent_global_id,
         navn,
         dekning_prc,
         creation_date,
         creator,
         edit_date,
         editor)
```

```{r}
dbSendStatement(con, "DELETE FROM ano.herb_species WHERE creator = 'Jens_Astrom_manual';")

dbWriteTable(con,
             Id(schema = "ano", table = "herb_species"),
             nina_ano_species,
             append = T)
```

Trees
------

```{r}
nina_ano_trees <- read.xlsx("../../Data/ANO_reg/ANO_punching_Jens.xlsx", 
                           detectDates = T,
                           sheet = 3) %>% 
  as_tibble()
```

Fix some column names
```{r}
old_names <- names(nina_ano_trees)
new_names <- tolower(old_names)
new_names <- gsub("\\.", "_", new_names)
new_names <- gsub(":", "", new_names)
new_names <- gsub("\\?", "", new_names)
new_names <- gsub("\\(", "", new_names)
new_names <- gsub("\\)", "", new_names)
new_names <- gsub("\\%", "prc", new_names)
new_names <- gsub("\\-", "_", new_names)
new_names <- gsub("\\/", "_", new_names)
new_names[new_names=="globalid"] <- "global_id"
new_names[new_names=="editdate"] <- "edit_date"
new_names[new_names=="creationdate"] <- "creation_date"
new_names[new_names=="parentglobalid"] <- "parent_global_id"
new_names
```

```{r}
names(nina_ano_trees) <- new_names
```

Select just the columns to be imported.

```{r}

nina_ano_trees <- nina_ano_trees %>% 
  select(global_id,
         parent_global_id,
         navn,
         creation_date,
         creator,
         edit_date,
         editor)
```

```{r}
dbSendStatement(con, "DELETE FROM ano.tree_species WHERE creator = 'Jens_Astrom_manual';")

dbWriteTable(con,
             Id(schema = "ano", table = "tree_species"),
             nina_ano_trees,
             append = T)
```


Problem species
-------

```{r}
nina_ano_problem <- read.xlsx("../../Data/ANO_reg/ANO_punching_Jens.xlsx", 
                           detectDates = T,
                           sheet = 4) %>% 
  as_tibble()
```

Fix some column names
```{r}
old_names <- names(nina_ano_problem)
new_names <- tolower(old_names)
new_names <- gsub("\\.", "_", new_names)
new_names <- gsub(":", "", new_names)
new_names <- gsub("\\?", "", new_names)
new_names <- gsub("\\(", "", new_names)
new_names <- gsub("\\)", "", new_names)
new_names <- gsub("\\%", "prc", new_names)
new_names <- gsub("\\-", "_", new_names)
new_names <- gsub("\\/", "_", new_names)
new_names[new_names=="globalid"] <- "global_id"
new_names[new_names=="editdate"] <- "edit_date"
new_names[new_names=="creationdate"] <- "creation_date"
new_names[new_names=="parentglobalid"] <- "parent_global_id"
new_names
```

```{r}
names(nina_ano_problem) <- new_names
```

Select just the columns to be imported.

```{r}

nina_ano_problem <- nina_ano_problem %>% 
  select(global_id,
         parent_global_id,
         navn,
         creation_date,
         creator,
         edit_date,
         editor)
```

```{r}
dbSendStatement(con, "DELETE FROM ano.problem_species WHERE creator = 'Jens_Astrom_manual';")

dbWriteTable(con,
             Id(schema = "ano", table = "problem_species"),
             nina_ano_problem,
             append = T)
```

Alien species
-------

```{r}
nina_ano_aliens <- read.xlsx("../../Data/ANO_reg/ANO_punching_Jens.xlsx", 
                           detectDates = T,
                           sheet = 5) %>% 
  as_tibble()
```

Fix some column names
```{r}
old_names <- names(nina_ano_aliens)
new_names <- tolower(old_names)
new_names <- gsub("\\.", "_", new_names)
new_names <- gsub(":", "", new_names)
new_names <- gsub("\\?", "", new_names)
new_names <- gsub("\\(", "", new_names)
new_names <- gsub("\\)", "", new_names)
new_names <- gsub("\\%", "prc", new_names)
new_names <- gsub("\\-", "_", new_names)
new_names <- gsub("\\/", "_", new_names)
new_names[new_names=="globalid"] <- "global_id"
new_names[new_names=="editdate"] <- "edit_date"
new_names[new_names=="creationdate"] <- "creation_date"
new_names[new_names=="parentglobalid"] <- "parent_global_id"
new_names
```

```{r}
names(nina_ano_aliens) <- new_names
```

Select just the columns to be imported.

```{r}

nina_ano_aliens <- nina_ano_aliens %>% 
  select(global_id,
         parent_global_id,
         navn,
         creation_date,
         creator,
         edit_date,
         editor)

```

```{r}
dbSendStatement(con, "DELETE FROM ano.alien_species WHERE creator = 'Jens_Astrom_manual';")

dbWriteTable(con,
             Id(schema = "ano", table = "alien_species"),
             nina_ano_aliens,
             append = T)


```

Weather data
========
The data exports for the temperature and humidity MX loggers from Hobo needs a bit of data wrangling before it can be used. The different data streams from each logger all get a separate column. Here we develop a script to turn this into a more usable long format. We also make tables in a database and upload the data there.

Read in data
==========
We have an export file from Hobolink.com with many loggers as a csv file. We also have some individual csv files that failed to upload to the Hobo site, that we'll handle later on.

```{r}
inputFile <- "../rawData/Insektoverv_k_2020_2020_11_10_11_56_24_UTC_1.csv"

rawDat <- read_csv(inputFile,col_types = cols(.default = "c"))

dat <- rawDat %>%  
  select(-"Line#") %>% 
  mutate(date = as.POSIXct(Date, format = "%m/%d/%y %H:%M:%S")) %>% 
  mutate_if(is_character, as.double) %>% 
  select(-Date)

dat
```
That's quite the number of columns...


We have to pivot this data set to a longer format. We also get rid of the rows with no data.
```{r}
temp <- dat %>% 
  pivot_longer(cols = starts_with("Temperature"),
               names_to = "logger_id",
               values_to = "temperature") %>% 
  select(date,
         logger_id,
         temperature) %>% 
  filter(!is.na(temperature))

rh <- dat %>% 
  pivot_longer(cols = starts_with("RH"),
               names_to = "logger_id",
               values_to = "rh") %>% 
  select(date,
         logger_id,
         rh)%>% 
  filter(!is.na(rh))

dew_point  <- dat %>% 
  pivot_longer(cols = starts_with("Dew"),
               names_to = "logger_id",
               values_to = "dew_point") %>% 
  select(date,
         logger_id,
         dew_point) %>% 
  filter(!is.na(dew_point))

```

The data now looks like this
```{r}
temp
```

Time to strip the logger names and merge the tables

```{r}
temp <- temp %>% 
  mutate(logger_id = str_extract(logger_id,
                              "[^, ]+$"))

rh <- rh %>% 
  mutate(logger_id = str_extract(logger_id,
                              "[^, ]+$"))
dew_point <- dew_point %>% 
  mutate(logger_id = str_extract(logger_id,
                              "[^, ]+$"))

```

Check to see that the dates are the same for the datasets
```{r}
all(all(temp$date == rh$date),
all(rh$date == dew_point$date))
```

```{r}
combDat <- temp %>% 
  full_join(rh,
             by = c("date" = "date",
                    "logger_id" = "logger_id")) %>% 
  full_join(dew_point,
            by = c("date" = "date",
                    "logger_id" = "logger_id")) %>% 
  arrange(logger_id,
          date) %>% 
    mutate(logger_type = "MX2301A") %>% 
    select(date, 
           logger_type,
           logger_id,
           temperature,
           rh,
           dew_point)
```


```{r}
combDat
```

Package this into a function
==========

```{r}
longerHobo2301 <- function(inputFile){
  
  rawDat <- read_csv(inputFile,col_types = cols(.default = "c"))

  dat <- rawDat %>%  
    select(-"Line#") %>% 
    mutate(date = as.POSIXct(Date, format = "%m/%d/%y %H:%M:%S")) %>% 
    mutate_if(is_character, as.double) %>% 
    select(-Date)


  temp <- dat %>% 
    pivot_longer(cols = starts_with("Temperature"),
               names_to = "logger_id",
               values_to = "temperature") %>% 
    select(date,
         logger_id,
         temperature) %>% 
    filter(!is.na(temperature))

  rh <- dat %>% 
    pivot_longer(cols = starts_with("RH"),
                 names_to = "logger_id",
                 values_to = "rh") %>% 
    select(date,
           logger_id,
           rh)%>% 
    filter(!is.na(rh))
  
  dew_point  <- dat %>% 
    pivot_longer(cols = starts_with("Dew"),
                 names_to = "logger_id",
                 values_to = "dew_point") %>% 
    select(date,
           logger_id,
           dew_point) %>% 
    filter(!is.na(dew_point))
  
  
  temp <- temp %>% 
    mutate(logger_id = str_extract(logger_id,
                              "[^, ]+$"))
  rh <- rh %>% 
    mutate(logger_id = str_extract(logger_id,
                                "[^, ]+$"))
  dew_point <- dew_point %>% 
    mutate(logger_id = str_extract(logger_id,
                                "[^, ]+$"))
  
  if(!all(all(temp$date == rh$date),
  all(rh$date == dew_point$date))) stop("Tables datetimes doesn't match")
  
  combDat <- temp %>% 
  full_join(rh,
             by = c("date" = "date",
                    "logger_id" = "logger_id")) %>% 
  full_join(dew_point,
            by = c("date" = "date",
                    "logger_id" = "logger_id")) %>% 
  arrange(logger_id,
          date) %>% 
    mutate(logger_type = "MX2301A") %>% 
    select(date, 
           logger_type,
           logger_id,
           temperature,
           rh,
           dew_point)
  
  return(combDat)
}
```

We can check that it produces the same results as the script.

```{r}
combDat2 <- longerHobo2301("../rawData/Insektoverv_k_2020_2020_11_10_11_56_24_UTC_1.csv")

all(combDat == combDat2)
```


Check the data out
=======
Some simple figures.

```{r}
ggplot(combDat2) +
  geom_line(aes(x = date, y = temperature, color = logger_id)) +
  ggtitle("All the temperatures so far")

```

```{r}
oneLogger <- combDat %>% 
  filter(logger_id == "20835815") %>% 
  select(Date = date, 
         logger_id,
         Temperature = temperature,
         Relative_humidity = rh,
         Dew_point = dew_point) %>% 
  pivot_longer(-c(Date, logger_id),
               names_to = "Data_type",
               values_to = "Values")
  
ggplot(oneLogger) +
  geom_line(aes(x = Date, y = Values, color = Data_type)) +
  scale_color_nina() +
  ggtitle("All the data from one logger")
```


Combine with single files from loggers that weren't synced
====================
We had some troubles with the uploads from HoboConnect to Hobolink.com from the CAT-phones. So the logger files from Oslo is provided individually by email. Time to combine these as well. These have a different data format than the export from hobolink. They also use comma as a decimal delimeter, as well as column delimeter, which complicates things. I think the HoboConnect app might pick the decimal delimiter from the locale of the phones, but doesn't adjust the colum delimiter.

```{r}

formatMX2301File <- function(inputFile){
raw <- read_csv(file = inputFile,
                col_types = cols(
  `#` = col_integer(),
  `Date-Time (CET)` = col_datetime(format = "%m.%d.%Y %H.%M.%S"),
  `Ch: 1 - Temperature  °C (°C)` = col_integer(),
  `Ch: 2 - RH  % (%)` = col_integer(),
  `Dew Point  °C (°C)` = col_integer(),
  `Button Down` = col_integer(),
  `Host Connected` = col_integer(),
  `End of File` = col_integer()
))

logger_id <- gsub("(.*/)([0-9]*)( .*)", "\\2", inputFile)
  
out <- raw %>% 
  filter(!is.na(`Ch: 1 - Temperature  °C (°C)`)) %>% 
  unite("temperature", `Ch: 1 - Temperature  °C (°C)`, `Ch: 2 - RH  % (%)`, sep = ".") %>% 
  unite("rh", `Dew Point  °C (°C)`, `Button Down`, sep = ".") %>% 
  unite("dew_point", `Host Connected`, `End of File`, sep = ".") %>% 
  mutate(logger_type = "MX2301A",
         date = `Date-Time (CET)`,
         logger_id = logger_id) %>% 
  select(date,
         logger_type,
         logger_id,
         temperature,
         rh,
         dew_point)

return(out)

}
```
```{r, message = F}
#logger_20835817 <- formatMX2301File("../rawData/20835817 2020-10-13 12_18_01 CET (Data CET).csv")
#logger_20835819 <- formatMX2301File("../rawData/20835819 2020-10-14 14_33_12 CET (Data CET).csv")
logger_20835820 <- formatMX2301File("../rawData/20835820 2020-10-16 14_09_17 CET (Data CET).csv")
#logger_20835821 <- formatMX2301File("../rawData/20835821 2020-10-15 15_49_08 CET (Data CET).csv")
#logger_20835823 <- formatMX2301File("../rawData/20835823 2020-10-15 12_11_02 CET (Data CET).csv")
#logger_20843228 <- formatMX2301File("../rawData/20843228 2020-10-14 11_43_58 CET (Data CET).csv")
#logger_20843229 <- formatMX2301File("../rawData/20843229 2020-10-16 09_59_17 CET (Data CET).csv")
#logger_20843233 <- formatMX2301File("../rawData/20843233 2020-10-14 16_25_22 CET (Data CET).csv")
#logger_20843238 <- formatMX2301File("../rawData/20843238 2020-10-16 16_52_35 CET (Data CET).csv")
```

Combine these files to the other ones.

```{r}
allMX2301 <- combDat2 %>% 
  #rbind(logger_20835817) %>% 
  #rbind(logger_20835819) %>% 
  rbind(logger_20835820) 
#%>% 
  #rbind(logger_20835821) %>% 
  #rbind(logger_20835823) %>% 
  #rbind(logger_20843228) %>% 
  #rbind(logger_20843229) %>% 
  #rbind(logger_20843233) %>% 
  #rbind(logger_20843238) 
  
```

Double check dew points
-------
This is a simplified formula for dew point, that seems to correspond fairly OK with the logger data. Not likely to be errors here.
```{r}
dewPoint <- function(input){
 
  input %>% 
    mutate(calc_dew_point = temperature - ((100 - rh)/5)) %>% 
  select(calc_dew_point)
}
```
```{r, eval = F}
combDat2
dewPoint(combDat2)

logger_20835817
dewPoint(logger_20835817)
```



Handle the MX2201 loggers
==========
These are temperature and light loggers that where also placed at some locations (that also had sound loggers). They have slightly different format, so we adapt the function to handle these.


```{r}
longerHobo2202 <- function(inputFile){
rawDat <- read_csv(inputFile,
                   guess_max = 10000,
                   col_types = cols())

  dat <- rawDat %>%  
    select(-"Line#") %>% 
    mutate(date = as.POSIXct(Date, format = "%m/%d/%y %H:%M:%S")) %>% 
    mutate_if(is_character, as.double) %>% 
    select(-Date)


   
  temp <- dat %>% 
    pivot_longer(cols = starts_with("Temperature"),
               names_to = "logger_id",
               values_to = "temperature") %>% 
    select(date,
         logger_id,
         temperature) %>% 
    filter(!is.na(temperature))

  light <- dat %>% 
    pivot_longer(cols = starts_with("Light"),
                 names_to = "logger_id",
                 values_to = "light") %>% 
    select(date,
           logger_id,
           light)%>% 
    filter(!is.na(light))
  
  
  
  temp <- temp %>% 
    mutate(logger_id = str_extract(logger_id,
                              "[^, ]+$"))
  light <- light %>% 
    mutate(logger_id = str_extract(logger_id,
                                "[^, ]+$"))

  if(!all(temp$date == light$date)) stop("Tables datetimes doesn't match")
  
  combDat <- temp %>% 
  full_join(light,
             by = c("date" = "date",
                    "logger_id" = "logger_id")) %>% 
  arrange(logger_id,
          date) %>% 
    mutate(logger_type = "MX2202") %>% 
    select(date, 
           logger_type,
           logger_id,
           temperature,
           light)
  
  return(combDat)
}
```

```{r}
allMX2202 <- longerHobo2202(inputFile = "../rawData/Insect_MX2202_temp_light_2020_10_27_13_02_59_UTC_1.csv")
```


Write the data to the database
==========
In the database, we combine the logger types into one table, and make it even longer. I.e. we combine all values in one column. (Many ways to do this).


```{r}
allMX2301Long <- allMX2301 %>% 
  pivot_longer(cols = c("temperature", "rh", "dew_point"),
               names_to = "data_type")

allMX2202Long <- allMX2202 %>% 
  pivot_longer(cols = c("temperature", "light"),
               names_to = "data_type")

allLoggersLong <- allMX2301Long %>% 
  rbind(allMX2202Long)

```


Make the database table
-----------------

```{r, eval = F}
createLoggerSchemaQ <- "
CREATE SCHEMA IF NOT EXISTS loggers;
"

createLoggerTableQ <- "
CREATE TABLE IF NOT EXISTS loggers.logger_data (
--id uuid NOT NULL DEFAULT gen_random_uuid(),
id serial NOT NULL,
date timestamptz NOT NULL,
logger_type text NOT NULL,
logger_id integer NOT NULL,
data_type text NOT NULL,
value double precision
);

"

dbSendQuery(con, createLoggerSchemaQ)
dbSendQuery(con, createLoggerTableQ)

dbSendQuery(con,
            "CREATE INDEX ON loggers.logger_data USING BTREE(date);")

dbSendQuery(con,
            "CREATE INDEX ON loggers.logger_data USING BTREE(logger_type);")

dbSendQuery(con,
            "CREATE INDEX ON loggers.logger_data USING BTREE(logger_id);")

dbSendQuery(con,
            "CREATE INDEX ON loggers.logger_data USING BTREE(data_type);")

```

```{r, eval = T}
dbSendStatement(con, "DELETE FROM loggers.logger_data;")

dbWriteTable(con, 
             name = Id(schema = "loggers", table = "logger_data"),
             value = allLoggersLong,
             append = T
             )

```

Read in logger deployments
```{r}
logger_deployments <- read_delim("../../../Data/klimalogger/logger_deployments_2020.csv",
                               delim = ";")
```


```{r, eval = F}
createLoggerDeploymentsQ <- "
CREATE TABLE IF NOT EXISTS loggers.logger_deployments (
--id uuid NOT NULL DEFAULT gen_random_uuid(),
id serial PRIMARY KEY,
logger_id integer NOT NULL,
logger_type text NOT NULL,
year integer NOT NULL,
location text NOT NULL
);
"
dbSendQuery(con, createLoggerDeploymentsQ)

dbSendQuery(con,
            "CREATE INDEX ON loggers.logger_deployments USING BTREE(logger_id);")

dbSendQuery(con,
            "CREATE INDEX ON loggers.logger_deployments USING BTREE(logger_type);")

dbSendQuery(con,
            "CREATE INDEX ON loggers.logger_deployments USING BTREE(year);")

dbSendQuery(con,
            "CREATE INDEX ON loggers.logger_deployments USING BTREE(location);")


```
```{r, eval = F}
dbWriteTable(con,
             name = Id(schema = "loggers", table = "logger_deployments"),
             value = logger_deployments,
             append = T
             )
```

Write the data as CSV
=============
We also write the data as csv files, if we don't want to use the database.

```{r}
write_csv(allLoggersLong, path = "../out/insectLogger_data_2020.csv")
write_csv(logger_deployments, path = "../out/insect_logger_deployments.csv")
```


Bottle weights
========
Here we've weighed samples of all bottle types to be able to subtract the mean weight of these from the recorded weights.

```{r}
bottle_weights <- read_delim("../../Data/Genetikkdata/Tomflaske_vekter.csv",
                             delim = ";",)

bottle_weights <- bottle_weights %>% 
  mutate(year = 2020)

```

```{r}
create_catch_schema <- "
CREATE SCHEMA IF NOT EXISTS catch_data;
"

dbSendStatement(con, create_catch_schema)

dbSendStatement(con, "
GRANT ALL ON SCHEMA catch_data TO ag_pgsql_radardata_admin;")

dbSendStatement(con, "GRANT USAGE ON SCHEMA catch_data TO ag_pgsql_radardata_ro;")

dbSendStatement(con, "GRANT USAGE ON SCHEMA catch_data TO ag_pgsql_radardata_rw;")

dbSendStatement(con, "ALTER DEFAULT PRIVILEGES IN SCHEMA catch_data
GRANT ALL ON TABLES TO ag_pgsql_radardata_admin;")

dbSendStatement(con, "ALTER DEFAULT PRIVILEGES IN SCHEMA catch_data
GRANT SELECT ON TABLES TO ag_pgsql_radardata_ro;")

dbSendStatement(con, "ALTER DEFAULT PRIVILEGES IN SCHEMA catch_data
GRANT INSERT, SELECT, UPDATE ON TABLES TO ag_pgsql_radardata_rw;")



create_bottle_weights <- "
CREATE TABLE catch_data.bottle_weights (
id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
bottle_type text,
bottle_nr integer,
year integer,
dry_weight double precision
)
"

dbSendStatement(con, create_bottle_weights)

dbWriteTable(con,
             name = Id(schema = "catch_data", table = "bottle_weights"),
             value = bottle_weights,
             append = T)

```

Import landsskogstaksering
============

```{r}
create_landsskog_schema <- "
CREATE SCHEMA IF NOT EXISTS landsskog;
"

dbSendStatement(con, create_landsskog_schema)

dbSendStatement(con, "
GRANT ALL ON SCHEMA landsskog TO ag_pgsql_radardata_admin;")

dbSendStatement(con, "GRANT USAGE ON SCHEMA landsskog TO ag_pgsql_radardata_ro;")

dbSendStatement(con, "GRANT USAGE ON SCHEMA landsskog TO ag_pgsql_radardata_rw;")

dbSendStatement(con, "ALTER DEFAULT PRIVILEGES IN SCHEMA landsskog
GRANT ALL ON TABLES TO ag_pgsql_radardata_admin;")

dbSendStatement(con, "ALTER DEFAULT PRIVILEGES IN SCHEMA landsskog
GRANT SELECT ON TABLES TO ag_pgsql_radardata_ro;")

dbSendStatement(con, "ALTER DEFAULT PRIVILEGES IN SCHEMA landsskog
GRANT INSERT, SELECT, UPDATE ON TABLES TO ag_pgsql_radardata_rw;")
```

```{r}
landsskog_points <- read.xlsx("../../Data/Landsskogstaksering/landskog_punching_JÅ.xlsx",
                             sheet = 1,
                             detectDates = T,) %>% 
  mutate(registreringsdato = as.Date(registreringsdato, format = "%d.%m.%Y"),
         klokkeslett_start = as_hms(convertToDateTime(klokkeslett_start, origin = "1899-12-30 00:00")),
         klokkeslett_slutt = as_hms(convertToDateTime(klokkeslett_slutt, origin = "1899-12-30 00:00"))) %>% 
  as_tibble()

landsskog_points
```


```{r, eval = F}
create_landsskog_points <- "
CREATE TABLE landsskog.survey_points (
id uuid PRIMARY KEY,
ano_flate_id text,
ano_punkt_id integer,
observer text,
registreringsdato date,
klokkeslett_start time,
klokkeslett_slutt time,
skogkarakter integer
)
"

dbSendStatement(con, create_landsskog_points)

dbSendStatement(con, "CREATE INDEX ON landsskog.survey_points USING BTREE(ano_flate_id);")
dbSendStatement(con, "CREATE INDEX ON landsskog.survey_points USING BTREE(ano_punkt_id);")
dbSendStatement(con, "CREATE INDEX ON landsskog.survey_points USING BTREE(registreringsdato);")

```

```{r, eval = F}
dbSendStatement(con, "DELETE FROM landsskog.survey_points;")

dbWriteTable(con,
             Id(schema = "landsskog", table = "survey_points"),
             landsskog_points,
             append = T)
```

```{r}
landsskog_data <- read.xlsx("../../Data/Landsskogstaksering/landskog_punching_JÅ.xlsx",
                             sheet = 2) 

landsskog_data
```


```{r}
landsskog_data <- landsskog_data %>% 
  mutate(Høydetre = as.logical(Høydetre),
         Boniteringstre = as.logical(Boniteringstre)) %>% 
  mutate(Høydetre = ifelse(is.na(Høydetre), F, Høydetre),
         Boniteringstre = ifelse(is.na(Boniteringstre), F, Boniteringstre)) %>% 
  select(id,
         parent_id,
         tre_nummer = Tre.nummer,
         treslag_kode = Treslag,
         diameter_breast_height = DBH,
         tilstand = Tilstand,
         kommentar = Kommentar,
         hoydetre = Høydetre,
         trehoyde = Trehøyde,
         boniteringstre = Boniteringstre,
         alder = Alder) %>% 
  as_tibble()
  
landsskog_data
```

```{r, eval = F}

create_landsskog_data <- "
CREATE TABLE landsskog.survey_data (
id uuid PRIMARY KEY,
parent_id uuid,
tre_nummer integer,
treslag_kode integer,
diameter_breast_height double precision,
tilstand integer,
kommentar text,
hoydetre boolean,
trehoyde double precision,
boniteringstre boolean,
alder double precision,
CONSTRAINT fk_landskog_point FOREIGN KEY(parent_id) 
REFERENCES landsskog.survey_points(id)
ON DELETE RESTRICT
ON UPDATE CASCADE
)
"


dbSendStatement(con, "DROP TABLE IF EXISTS landsskog.survey_data;")

dbSendStatement(con, create_landsskog_data)

dbSendStatement(con, "CREATE INDEX ON landsskog.survey_data USING BTREE(parent_id);")
dbSendStatement(con, "CREATE INDEX ON landsskog.survey_data USING BTREE(treslag_kode);")
dbSendStatement(con, "CREATE INDEX ON landsskog.survey_data USING BTREE(hoydetre);")
dbSendStatement(con, "CREATE INDEX ON landsskog.survey_data USING BTREE(boniteringstre);")

```



```{r, eval = F}
dbSendStatement(con, "DELETE FROM landsskog.survey_data;")

dbWriteTable(con,
             Id(schema = "landsskog", table = "survey_data"),
             landsskog_data,
             append = T)
```

Tree-codes
```{r}
tree_codes <- read_csv("../../Data/Landsskogstaksering/trekoder.txt") %>% 
  select(treslag = TRESLAG,
         treslag_kode = SKJEMAKODE,
         treslag_definisjon = DEFINISJON) %>% 
  mutate(treslag = str_to_title(treslag))
tree_codes %>% print(n = Inf)

```

```{r, eval = F}

create_landsskog_treslag <- "
CREATE TABLE landsskog.tree_species (
id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
treslag text,
treslag_kode integer,
treslag_definisjon text
);
"

dbSendStatement(con, "DROP TABLE IF EXISTS landsskog.tree_species;")

dbSendStatement(con, create_landsskog_treslag)

dbSendStatement(con, "CREATE INDEX ON landsskog.tree_species USING BTREE(treslag);")


```
```{r, eval = F}

dbWriteTable(con,
             Id(schema = "landsskog", table = "tree_species"),
             tree_codes,
             append = T)
```

Import catch data to database
=========
This has some add-hoc corrections that is specific to this data set.



```{r import_csv}

dat <- read_delim("../../Data/Genetikkdata/long_format_data_GCF-2020-735_736_743_744_748_23112020.txt", 
                  delim= ";",
                  na = c("NA",
                         "N/A",
                         "Maa veies",
                         "Smaa flasker maa veies"),
                 col_types = cols(
  kingdom = col_character(),
  phylum = col_character(),
  class = col_character(),
  order = col_character(),
  family = col_character(),
  genus = col_character(),
  species = col_character(),
  sample = col_character(),
  value = col_double(),
  GenlabID = col_character(),
  GUID = col_character(),
  mottatt_lab = col_character(),
  locality = col_character(),
  trap = col_character(),
  date_placed = col_date(format = "%d.%m.%Y"),
  time_placed = col_time(format = ""),
  empty_week = col_double(),
  date_emptied = col_date(format = "%d.%m.%Y"),
  time_emptied = col_time(format = ""),
  liquid = col_character(),
  person_emptied = col_character(),
  sample_id = col_character(),
  comment = col_character(),
  Tilsatt_antall_C.analis_SI_USA_08.06.20 = col_double(),
  Tilsatt_antall_C.chinensis_08.06.20 = col_double(),
  Tilsatt_antall_Sirisser = col_double(),
  Tilsatt_antall_Melormer = col_double(),
  Tilsatt_antall_C.maculatus = col_double(),
  Vekt_tom_flaske_.g. = col_double(),
  Vekt_flaske_med_EtOH_og_insekter_.g. = col_double(),
  Vekt_flaske_._insekter_vaat_.g. = col_double(),
  Vekt_flaske_._insekter_toerr_.g. = col_double(),
  Totalvolum_.mengde_ATL_._prot.K_mL. = col_double(),
  Vaatvekt_.g. = col_double(),
  Toerrvekt_.g. = col_double(),
  Kommentar_proeve = col_character(),
  Ekstraksjonsdato = col_date(format = "%d.%m.%Y"),
  Ekstraksjonskit = col_character(),
  Subsamplet_volum_.ul. = col_double(),
  Elueringsvolum = col_double(),
  OD = col_double(),
  X260_280 = col_double(),
  X260_230 = col_double(),
  Labperson = col_character(),
  Ekstraksjonskommentar = col_character(),
  kommentar_genlab = col_character(),
  run = col_character(),
  taxonomic_level = col_character(),
  identification_confidence = col_character(),
  possible_marker_issue = col_logical(),
  probable_database_insufficiency = col_logical(),
  norsk = col_logical(),
  antall_arter_i_norge = col_double(),
  antall_norsk_arter_i_db = col_double(),
  pct_db_coverage = col_double()
)
)


```
```{r, eval = F, echo = F}
spec(dat)
```

Fix some coding issues.
```{r fix_coding}
# dat %>% 
#   select(genus, species, new_species) %>% 
# slice(297, 1902)
#   
# 
# dat %>% 
#   select(genus, species, new_species) %>% 
# slice(3219, 3585, 4017, 11424, 13275, 15465, 21056, 24168, 25077, 31567, 32805, 48502, 49967)
#   
#Ekstraksjonskommentar = gsub(pattern = "[\\\"]", "", Ekstraksjonskommentar),

dat <- dat %>% 
  mutate(species_latin = species)

dat <- dat %>% 
  mutate(species = gsub(" sp", "_sp", species))

dat <- dat %>% 
  mutate(species = gsub("([0-9])(_)(sp)", "\\1/\\3", species))

dat <- dat %>%
  mutate(species = gsub("(.*)(_)(.*)", "\\3", species))

dat <- dat %>% 
  mutate(locality = str_to_sentence(locality))

```

Change spelling to norwegian
---------

```{r}
dat <- dat %>% 
  mutate(locality = ifelse(locality == "Nerlandsoeya", "Nerlandsøya", locality))
```

!!Not done
Fix trap names from M1 to MF1

Set factor levels

```{r}
dat <- dat %>%
  mutate(locality = factor(locality, levels = c(paste0("Semi-nat_", 1:10), paste0("Skog_", 1:10), "Nerlandsøya")),
         trap_type = ifelse(grepl("MF", trap), "Malaise", "Vindu"),
         habitat_type = ifelse(grepl("Skog", locality), "Skog", "SN_Gressmark"),
         liquid = ifelse(grepl("etoh85", liquid), "Ethanol", "PropGl_Ethanol"),
         weeks_sampled = ifelse(grepl("1", trap) | grepl("3", trap), 2, 4)
         )
```



Add missing bottle weights. This is based on a sample of bottle dry weight measurements made after the regular weighings. 

```{r}
bottle_weights <- tbl(con,
                   in_schema("catch_data", "bottle_weights")) %>% 
  select(-id)


# bottle_weights %>% 
#   filter(bottle_type == "MF" & year == 2000) %>% 
#   select(dry_weight) %>% 
#   ggplot(.) +
#   geom_density(aes(x = dry_weight))

bottle_weights_MF <- bottle_weights %>% 
  filter(bottle_type == "MF" & year == 2000) %>% 
  select(dry_weight) %>% 
  summarize(mean_weight = mean(dry_weight)) %>% 
  collect()


bottle_weights_VF_stor <- bottle_weights %>% 
  filter(bottle_type == "VF_stor" & year == 2000) %>% 
  select(dry_weight) %>% 
  summarize(mean_weight = mean(dry_weight)) %>% 
  collect()

bottle_weights_VF_liten <- bottle_weights %>% 
  filter(bottle_type == "VF_liten" & year == 2000) %>% 
  select(dry_weight) %>% 
  summarize(mean_weight = mean(dry_weight)) %>% 
  collect()

```
Add mean bottle weights for the missing MF bottles. 
```{r}
sum(is.na(dat$Vekt_tom_flaske_.g.))
dat <- dat %>% 
  mutate(Vekt_tom_flaske_.g. = ifelse(trap_type == "Malaise" & is.na(Vekt_tom_flaske_.g.), bottle_weights_MF$mean_weight, Vekt_tom_flaske_.g.))
   
sum(is.na(dat$Vekt_tom_flaske_.g.))
```

The window traps is a little bit trickier, since we might use different bottle sizes. We have to add the big bottle weights to the samples that plausibly used big bottles, i.e. dry weights around the big bottle weights. Here we use a cutoff for the bottle types of 50 grams.

```{r}
dat <- dat %>% 
  mutate(Vekt_tom_flaske_.g. = ifelse(trap_type == "Vindu" & is.na(Vekt_tom_flaske_.g.) & Vekt_flaske_._insekter_toerr_.g. > 50, bottle_weights_VF_stor$mean_weight, Vekt_tom_flaske_.g.))
sum(is.na(dat$Vekt_tom_flaske_.g.))
```

Add the small bottle weights.
```{r}
dat <- dat %>% 
  mutate(Vekt_tom_flaske_.g. = ifelse(trap_type == "Vindu" & is.na(Vekt_tom_flaske_.g.) & Vekt_flaske_._insekter_toerr_.g. < 50, bottle_weights_VF_liten$mean_weight, Vekt_tom_flaske_.g.))
sum(is.na(dat$Vekt_tom_flaske_.g.))
```


```{r}
dat <- dat %>% 
     mutate(Vaatvekt_.g. = Vekt_flaske_._insekter_vaat_.g. - Vekt_tom_flaske_.g.,
            Toerrvekt_.g. = Vekt_flaske_._insekter_toerr_.g. - Vekt_tom_flaske_.g.)
```

Fix some dates

```{r}
dat %>% 
  filter(date_placed < "2020-01-01")
```


Add the missing Nerlandsøya data (earlier versions was lacking this at least.)

```{r}
dat <- dat %>% 
  mutate(date_placed = replace(date_placed, (locality == "Nerlandsøya" &
           date_emptied == "2020-06-13"), "2020-05-22"))

dat <- dat %>% 
  mutate(date_placed = replace(date_placed, (locality == "Nerlandsøya" &
           date_emptied == '2020-06-30'), '2020-06-13'))

dat <- dat %>% 
  mutate(sample_id = ifelse((locality == "Nerlandsøya" &
           date_emptied == '2020-06-13'), 'nerlandsøya_MF1_week_24', sample_id))

dat <- dat %>% 
  mutate(sample_id = ifelse((locality == "Nerlandsøya" &
           date_emptied == '2020-06-30'), 'nerlandsøya_MF1_week_27', sample_id))

dat %>% 
  select(date_placed) %>% 
  distinct()

dat %>% 
  filter(locality == "Nerlandsøya" &
           date_emptied == '2020-06-13') %>% 
  select(date_placed)
```




Check the emptying dates.
----------------
The dates seems to be OK this time. Should do a proper check on the endpoints though.
```{r}
dat <- dat %>% 
  mutate(days_collected = as.numeric(dat$date_emptied - dat$date_placed))

# dat %>% 
#   filter(days_collected == 0) %>% 
#   select(sample_id,
#          GUID,
#          date_placed,
#          date_emptied) %>% 
#   distinct() %>% 
#   print(n = Inf)
# 
# hist(as.numeric(dat$date_emptied - dat$date_placed))
```

<!-- These traps have erroneous placement dates. -->
<!-- ```{r} -->
<!-- dat %>%  -->
<!--   filter(sample_id == "skog_1_MF1_week_") %>%  -->
<!--   slice(1:2) %>%  -->
<!--   print(width = Inf) -->

<!-- dat %>%  -->
<!--   filter(sample_id == "skog_1_MF1_week_26") -->
<!-- ``` -->

```{r}

dat %>% 
  group_by(days_collected) %>% 
  summarise(n())

# dat %>% 
#   filter(days_collected == 8) %>% 
#   select(sample_id,
#          GUID,
#          date_placed,
#          date_emptied) %>% 
#   distinct() %>% 
#   print(n = Inf)
```

Fix a few typing errors in the dates. This is updated now in the sources, and should be fixed in the next data version.

```{r}
dat %>% 
  filter(days_collected <1) %>% 
  select(locality, sample_id, trap, date_placed, date_emptied) %>% 
  distinct()
```
```{r}
dat <- dat %>% 
  mutate(date_placed = replace(date_placed, sample_id == "semi-nat_2_MF1_week_34", as.Date("2020-08-05"))) %>% 
  mutate(date_placed = replace(date_placed, sample_id == "semi-nat_2_MF1_week_36", as.Date("2020-08-19"))) %>% 
  mutate(date_emptied = replace(date_emptied, sample_id == "semi-nat_6_MF1_week_34", as.Date("2020-08-19"))) %>% 
  mutate(date_emptied = replace(date_emptied, sample_id == "semi-nat_6_MF2_week_34", as.Date("2020-08-19"))) %>%  
  mutate(date_placed = replace(date_placed, sample_id == "semi-nat_7_MF1_week_36", as.Date("2020-08-19"))) %>% 
  mutate(date_placed = replace(date_placed, sample_id == "semi-nat_8_MF1_week_36", as.Date("2020-08-19"))) %>% 
  mutate(date_placed = replace(date_placed, sample_id == "skog_10_MF1_week_36", as.Date("2020-08-18"))) %>% 
  mutate(date_placed = replace(date_placed, sample_id == "skog_10_VF1_week_36", as.Date("2020-08-18")))  %>% 
  mutate(date_placed = replace(date_placed, sample_id == "skog_10_VF3_week_36", as.Date("2020-08-18")))  %>% 
  mutate(date_placed = replace(date_placed, sample_id == "skog_5_MF1_week_36", as.Date("2020-08-19"))) %>% 
  mutate(date_placed = replace(date_placed, sample_id == "skog_5_VF1_week_36", as.Date("2020-08-19")))%>% 
  mutate(date_placed = replace(date_placed, sample_id == "skog_5_VF3_week_36", as.Date("2020-08-19"))) %>% 
  mutate(date_placed = replace(date_placed, sample_id == "skog_8_MF1_week_36", as.Date("2020-08-18"))) 

```


```{r}
dat <- dat %>% 
  mutate(days_collected = as.numeric(dat$date_emptied - dat$date_placed))
```

```{r}
dat %>% 
  filter(days_collected <1) %>% 
  select(locality, sample_id, trap, date_placed, date_emptied) %>% 
  distinct()
```

Check the NA values for days_collected. Only the Nerlandsøya samples lacks some data.
```{r}
dat %>% 
  filter(is.na(days_collected)) %>% 
  select(locality, sample_id, trap, date_placed, date_emptied) %>% 
  distinct()
```



Set weeks sampled manually for the few locations in Oslo that got an extra short sampling in the beginning.
```{r sample_days_check}
ggplot(dat, aes(x = weeks_sampled, y = days_collected, group = weeks_sampled)) +
  geom_boxplot()

```


```{r}
twoWeeks <- dat %>% 
  filter(weeks_sampled == 4 &
           days_collected < 20) %>% 
  select(sample_id) %>% 
  distinct() %>% 
  as.vector()


dat <- dat %>% 
  mutate(weeks_sampled = ifelse(sample_id %in% twoWeeks[[1]], 2, weeks_sampled))


```

```{r}
ggplot(dat, aes(x = weeks_sampled, y = days_collected, group = weeks_sampled)) +
  geom_boxplot()

```

```{r}
dat %>% 
  filter(days_collected <1) %>% 
  select(date_placed, date_emptied) %>% 
  distinct()
```



Split up the sampling data and the observations.
------------
This is a bit ad-hoc now the first year, because we haven't set up the database in advance. In the future, the metadata will already be set up and not fed by the data from the metabarcoding.

```{r} 
metadata <- dat %>% 
  select(sample,
         GenlabID,
         GUID,
         mottatt_lab,
         locality,
         trap,
         date_placed,
         time_placed,
         date_emptied,
         time_emptied,
         person_emptied,
         liquid,
         empty_week,
         sample_id,
         comment,
         Tilsatt_antall_C.analis_SI_USA_08.06.20,
         Tilsatt_antall_C.chinensis_08.06.20,
         Tilsatt_antall_Sirisser,
         Tilsatt_antall_Melormer,
         Tilsatt_antall_C.maculatus,
         Vekt_tom_flaske_.g.,
         Vekt_flaske_med_EtOH_og_insekter_.g.,
         Vekt_flaske_._insekter_vaat_.g.,
         Vekt_flaske_._insekter_toerr_.g.,
         Totalvolum_.mengde_ATL_._prot.K_mL.,
         Vaatvekt_.g.,
         Toerrvekt_.g.,
         Kommentar_proeve,
         Ekstraksjonsdato,
         Ekstraksjonskit,
         Subsamplet_volum_.ul.,
         Elueringsvolum,
         OD,
         X260_280,
         X260_230,
         Labperson,
         Ekstraksjonskommentar,
         kommentar_genlab,
         run,
         trap_type,
         habitat_type,
         weeks_sampled,
         days_collected) %>% 
  distinct()


```

```{r}
metadata %>% 
  summarize(n_distinct(sample_id))

metadata %>% 
  summarize(n_distinct(sample))

```

```{r}
obs_data <- dat %>% 
  select(!c(GenlabID,
         GUID,
         mottatt_lab,
         locality,
         trap,
         date_placed,
         time_placed,
         date_emptied,
         time_emptied,
         person_emptied,
         liquid,
         empty_week,
         sample_id,
         comment,
         Tilsatt_antall_C.analis_SI_USA_08.06.20,
         Tilsatt_antall_C.chinensis_08.06.20,
         Tilsatt_antall_Sirisser,
         Tilsatt_antall_Melormer,
         Tilsatt_antall_C.maculatus,
         Vekt_tom_flaske_.g.,
         Vekt_flaske_med_EtOH_og_insekter_.g.,
         Vekt_flaske_._insekter_vaat_.g.,
         Vekt_flaske_._insekter_toerr_.g.,
         Totalvolum_.mengde_ATL_._prot.K_mL.,
         Vaatvekt_.g.,
         Toerrvekt_.g.,
         Kommentar_proeve,
         Ekstraksjonsdato,
         Ekstraksjonskit,
         Subsamplet_volum_.ul.,
         Elueringsvolum,
         OD,
         X260_280,
         X260_230,
         Labperson,
         Ekstraksjonskommentar,
         kommentar_genlab,
         run,
         trap_type,
         habitat_type,
         weeks_sampled,
         days_collected)) 

obs_data <- obs_data %>% 
  select(sample,
         kingdom,
         phylum,
         class,
         order,
         family,
         genus,
         species,
         species_latin,
         taxonomic_level,
         identification_confidence,
         possible_marker_issue,
         probable_database_insufficiency,
         norsk,
         antall_arter_i_norge,
         antall_norsk_arter_i_db,
         pct_db_coverage,
         value)

```

```{r}
dbWriteTable(con,
             Id(schema = "catch_data", table = "metabarcoding"),
             obs_data,
             overwrite = T)
```

```{r}
metadata <- metadata %>% 
  select(sample,
         sample_id,
         GenlabID,
         GUID,
         location = locality,
         habitat_type,
         trap_type,
         liquid,
         trap,
         date_placed,
         time_placed,
         date_emptied,
         time_emptied,
         person_emptied,
         empty_week,
         weeks_sampled,
         days_sampled = days_collected,
         mottatt_lab,
         Vekt_tom_flaske_.g.,
         Vekt_flaske_med_EtOH_og_insekter_.g.,
         Vekt_flaske_._insekter_vaat_.g.,
         Vekt_flaske_._insekter_toerr_.g.,
         Vaatvekt_.g.,
         Toerrvekt_.g.,
         Tilsatt_antall_C.analis_SI_USA_08.06.20,
         Tilsatt_antall_C.chinensis_08.06.20,
         Tilsatt_antall_Sirisser,
         Tilsatt_antall_Melormer,
         Tilsatt_antall_C.maculatus,
         Ekstraksjonsdato,
         Ekstraksjonskit,
         Subsamplet_volum_.ul.,
         Elueringsvolum,
         Totalvolum_.mengde_ATL_._prot.K_mL.,
         OD,
         run,
         X260_280,
         X260_230,
         Labperson,
         Ekstraksjonskommentar,
         kommentar_genlab,
         Kommentar_proeve,
         comment
         )
```

```{r}
dbWriteTable(con,
             Id(schema = "catch_data", table = "sample_metadata"),
             metadata,
             overwrite = T)
```

```{r}

add_uq <- "
ALTER TABLE catch_data.sample_metadata
ADD CONSTRAINT sample_uq
UNIQUE(sample);
"

add_fk <- "
ALTER TABLE catch_data.metabarcoding
ADD CONSTRAINT sample_fk
FOREIGN KEY(sample)
REFERENCES catch_data.sample_metadata (sample)
ON DELETE RESTRICT
ON UPDATE CASCADE;

"
dbSendStatement(con, add_uq)
dbSendStatement(con, add_fk)

```

This data contains no information of the intraspecific variation.
```{r}
genotype_check <- dat %>% 
  group_by(sample) %>% 
  summarize(no_species = n_distinct(species_latin),
            no_rows = n()) %>%
  mutate(diff = no_species - no_rows)
              
all(genotype_check$diff == 0)
```

Write an outline of the export view(s) to GBIF. Need to work out some details with Frode and Marie about ASV reporting to GBIF. 

* ASVs as MD5 checksums?
* Read amount for each ASV, plus total read abundance for entire sample?
* How likely are we to use multiple PCRs of the same sample? Do we need a hierarchical level for the various identifications of the same sample? Perhaps good, to facilitate manual identification in future?


Data structure plan for dissemination through GBIF
============
We aim to conform to the FAIR-principle as far as possible, for the storage and dissemination of our data. This means that the captured information should be stored at the highest granularity possible, and that the structure of the data should be retrievable. In practice, we shouldn't aggregate the data more than necessary, and we should incorporate the hierarchical structure of the data, e.g. the repeated measurements within a site, study period, trap and identification. 

The structure of the data might change over time, given that we might have fewer number of traps per site, and only operate with a single sampling scheme at each site etc. But in principle, this could change in either direction in the future, so we better make room for this information. These levels of sampling seems to be relevant:

1. year_location_event.
  - This should be defined as the entire 500x500 square since the trap locations might differ or change
  - The location should be given a UUID as well as a vernacular name, and be provided with a polygon geometry.
  - This can also incorporate information on the habitat type.
  - Associated environmental data can be joined to this unit, and linked to the sample square in a **"sample square event"** that spans the entire season. Not sure of how much of this to include in GBIF
  - Dates are the start of the first sampling event and the end of the last sampling event.
2. location_sampling_event
  - This is unique for every sample square and collection period. Multiple traps with the same time period in the same location share the same trapping event.
  - Temperature data can be linked to this.
  - can be used to separate two collection period that share a start XOR end time.
  - no geometry at this level
3. sampling_trap_event
  - Each trap within the same collection time and place gets their separate trap event.
  - Include information about location as high definition GPS, trap type and storage medium.
  - Individual traps are kept separate, if handled separately (not combined) in identification.
4. Identification events.
  - Include information about the identification process
  - Could have multiple runs of the metabarcoding, or multiple identification methodologies for the same sampling trap event (manual vs. metabarcoding)
  - Could include identification effort (total read abundance, scope of manual identification)
5. Occurrences within identification events.
  - Should include linnean taxonomy plus ASV, plus amount of reads for each record for metabarcoding, caste, sex etc.
  
6. Various tables of related "environmental" information that is linked to these respective levels (1-5).
  
Probably neater to have separate tables for each event hierarchy. So that these UUIDs get set once and for all, and views can refer to.


Create sample square event view
----------------

First, set up a schema with foreign tables. Add ssb 500 grid. 

```{r}

create_foreign_schema <- "
CREATE SCHEMA IF NOT EXISTS foreign_tables
AUTHORIZATION ag_pgsql_radardata_admin;
"
dbSendStatement(con, create_foreign_schema)


dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA foreign_tables
GRANT ALL ON TABLES TO ag_pgsql_radardata_admin;
")

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA foreign_tables
GRANT SELECT ON TABLES TO ag_pgsql_radardata_ro;
")

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA foreign_tables
GRANT INSERT, SELECT, UPDATE ON TABLES TO ag_pgsql_radardata_rw;

")

set_up_foreign_server <- "
CREATE SERVER \"gisdata-db\"
FOREIGN DATA WRAPPER postgres_fdw
OPTIONS (host 'gisdata-db.nina.no', port '5432', dbname 'gisdata', use_remote_estimate 'true')
"

alt_owner_fs <- "
ALTER SERVER \"gisdata-db\"
    OWNER TO ag_pgsql_radardata_admin;

"
set_mapping_fs <- "
CREATE USER MAPPING FOR \"jens.astrom\" SERVER \"gisdata-db\"
    OPTIONS (password 'gjestpost', \"user\" 'postgjest');
"

dbSendStatement(con, set_up_foreign_server)
dbSendStatement(con, alt_owner_fs)
dbSendStatement(con, set_mapping_fs)


create_foreign_tables_ssbid <-  "
CREATE FOREIGN TABLE foreign_tables.ssb_500m(
    ssbid bigint NOT NULL,
    rsize integer,
    \"row\" integer,
    col integer,
    xcoor integer,
    ycoor integer,
    geom geometry(MultiPolygon,25833)
)
    SERVER \"gisdata-db\"
    OPTIONS (schema_name 'ssb_data_utm33n', table_name 'ssb_500m');"


dbSendStatement(con, create_foreign_tables_ssbid)

dbSendStatement(con, "
GRANT ALL ON TABLE foreign_tables.ssb_500m TO ag_pgsql_radardata_admin;
")

dbSendStatement(con, "
GRANT SELECT ON TABLE foreign_tables.ssb_500m TO ag_pgsql_radardata_ro;
")

dbSendStatement(con, "
GRANT INSERT, SELECT, UPDATE ON TABLE foreign_tables.ssb_500m TO ag_pgsql_radardata_rw;
")



```


```{r}
create_view_schema <- "
CREATE SCHEMA IF NOT EXISTS views
AUTHORIZATION ag_pgsql_radardata_admin;
"

dbSendStatement(con, create_view_schema)

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA views
GRANT ALL ON TABLES TO ag_pgsql_radardata_admin;
")

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA views
GRANT SELECT ON TABLES TO ag_pgsql_radardata_ro;
")

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA views
GRANT INSERT, SELECT, UPDATE ON TABLES TO ag_pgsql_radardata_rw;
")

```


```{r}
create_lookup_schema <- "
CREATE SCHEMA IF NOT EXISTS lookup
AUTHORIZATION ag_pgsql_radardata_admin;
"

dbSendStatement(con, create_lookup_schema)

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA lookup
GRANT ALL ON TABLES TO ag_pgsql_radardata_admin;
")

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA lookup
GRANT SELECT ON TABLES TO ag_pgsql_radardata_ro;
")

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA lookup
GRANT INSERT, SELECT, UPDATE ON TABLES TO ag_pgsql_radardata_rw;
")

```



```{r}
create_event_schema <- "
CREATE SCHEMA IF NOT EXISTS events
AUTHORIZATION ag_pgsql_radardata_admin;
"

dbSendStatement(con, create_event_schema)

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA events
GRANT ALL ON TABLES TO ag_pgsql_radardata_admin;
")

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA events
GRANT SELECT ON TABLES TO ag_pgsql_radardata_ro;
")

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA events
GRANT INSERT, SELECT, UPDATE ON TABLES TO ag_pgsql_radardata_rw;
")

```




Set up the sample square event view
```{r}
 
create_year_location_event_table <- "
CREATE TABLE events.year_location (
id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
ssbid bigint,
ano_id text,
location text,
habitat_type text,
year integer,
start_date timestamptz,
end_date timestamptz,
geom geometry(polygon, 25833)

)
"

dbSendStatement(con,
                create_year_location_event_table)

insert_year_location_data <- "
INSERT INTO events.year_location (
SELECT  l.id, l.ssbid, l.ano_flate_id as ano_id, l.location, l.habitat_type, l.year, min(date_placed) as start_date, max(date_emptied) as end_date, ST_GeometryN(s.geom, 1) as geom
FROM catch_data.sample_metadata sm LEFT JOIN
locations.locations l on 
(sm.location = l.location AND l.year = date_part('year', sm.date_placed))
LEFT JOIN
foreign_tablesssb_500m s
ON l.ssbid = s.ssbid
GROUP BY l.id, l.year, s.geom
ORDER BY l.year, l.location)
"

dbSendStatement(con,
                insert_year_location_data)
```

Set up names

```{r}
create_people_schema <- "
CREATE SCHEMA IF NOT EXISTS people
AUTHORIZATION ag_pgsql_radardata_admin;
"

dbSendStatement(con, create_people_schema)

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA people
GRANT ALL ON TABLES TO ag_pgsql_radardata_admin;
")

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA people
GRANT SELECT ON TABLES TO ag_pgsql_radardata_ro;
")

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA people
GRANT INSERT, SELECT, UPDATE ON TABLES TO ag_pgsql_radardata_rw;
")
```

```{r}
create_names <- "
CREATE TABLE lookup.names (
id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
full_name text NOT NULL,
abbrev text NOT NULL,
organisation text,
telephone text,
email text,
comment text,
CONSTRAINT names_uq UNIQUE(abbrev),
CONSTRAINT abbrev_uq UNIQUE(full_name)
)
"
dbSendStatement(con, "DROP TABLE IF EXISTS lookup.names")
dbSendStatement(con, create_names)

dbSendStatement(con, "INSERT INTO lookup.names SELECT * FROM people.names")

```





Set up location_sampling


```{r}

create_location_sampling_event_table <- "
CREATE TABLE events.location_sampling (
id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
year_location_id UUID NOT NULL,
start_date timestamptz,
end_date timestamptz,
started_by text,
ended_by text,
comment text,
CONSTRAINT year_location_fk 
FOREIGN KEY (year_location_id) REFERENCES events.year_location(id)
ON DELETE RESTRICT
ON UPDATE CASCADE,
CONSTRAINT started_name_fk 
FOREIGN KEY (started_by) REFERENCES lookup.names(abbrev)
ON DELETE RESTRICT
ON UPDATE CASCADE,
CONSTRAINT ended_by_fk
FOREIGN KEY (ended_by) REFERENCES lookup.names(abbrev)
ON DELETE RESTRICT
ON UPDATE CASCADE
)
"

dbSendStatement(con, "DROP TABLE IF EXISTS events.location_sampling CASCADE")


dbSendStatement(con,
                create_location_sampling_event_table)

```


```{r}
create_traps_schema <- "
CREATE SCHEMA IF NOT EXISTS traps
AUTHORIZATION ag_pgsql_radardata_admin;
"

dbSendStatement(con, create_traps_schema)

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA traps
GRANT ALL ON TABLES TO ag_pgsql_radardata_admin;
")

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA traps
GRANT SELECT ON TABLES TO ag_pgsql_radardata_ro;
")

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA traps
GRANT INSERT, SELECT, UPDATE ON TABLES TO ag_pgsql_radardata_rw;
")
```



```{r}

create_trap_types <- "
CREATE TABLE lookup.trap_types (
id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
trap_model text NOT NULL UNIQUE,
trap_type text NOT NULL,
trap_color text NOT NULL,
trap_manufacturer text NOT NULL
)
"
dbSendStatement(con, "DROP TABLE IF EXISTS lookup.trap_types CASCADE")
dbSendStatement(con, create_trap_types)
```

```{r}
create_liquid <- "
CREATE TABLE lookup.liquid (
id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
liquid_name text NOT NULL UNIQUE,
liquid_composition TEXT NOT NULL
)
"
dbSendStatement(con, "DROP TABLE IF EXISTS lookup.liquid")
dbSendStatement(con, create_liquid)
dbSendStatement(con, "COMMENT ON COLUMN lookup.liquid.liquid_name IS 'Short name';")
dbSendStatement(con, "COMMENT ON COLUMN lookup.liquid.liquid_composition IS 'Full composition';")

```



```{r}
create_trap_names <- "
CREATE TABLE traps.trap_names (
id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
trap_name text NOT NULL,
location text NOT NULL,
year integer NOT NULL,
trap_short_name text NOT NULL,
trap_model text ,
liquid_name text ,
coordinate_precision_m bigint,
elev_data boolean DEFAULT False,
geom Geometry(PointZ, 25833),
CONSTRAINT trap_model_fk 
FOREIGN KEY(trap_model) REFERENCES lookup.trap_types(trap_model)
ON DELETE RESTRICT
ON UPDATE CASCADE,
CONSTRAINT trap_liquid_fk 
FOREIGN KEY(liquid_name) REFERENCES lookup.liquid(liquid_name)
ON DELETE RESTRICT
ON UPDATE CASCADE
)
"
dbSendStatement(con, "DROP TABLE IF EXISTS traps.trap_names CASCADE")
dbSendStatement(con, create_trap_names)
```

Populate trap names

```{r}
pop_trap_names <- "
INSERT INTO traps.trap_names (trap_name, location, year, 
							  trap_short_name, coordinate_precision_m,
							 elev_data, geom)
SELECT 'Ost_' || '2020_' || location || '_' || trap as trap_name,
location, year, trap as trap_short_name, 
CASE WHEN gps_type = 'high_precision' THEN 
 0.05 ELSE
 5 END as coordinate_precision_m,
elev_data,
geom
FROM locations.trap_locations
"
dbExecute(con, pop_trap_names)

fill_extra_trap_col1 <- "
UPDATE traps.trap_names
SET trap_model = 'Malaise_Watkins_Black'
WHERE trap_short_name  LIKE 'MF%'
"
dbExecute(con, fill_extra_trap_col1) 
 
fill_extra_trap_col2 <- "
UPDATE traps.trap_names
SET trap_model = 'Window_NMBU_2020'
WHERE trap_short_name LIKE 'VF%'
"
dbExecute(con, fill_extra_trap_col2)  

fill_extra_trap_col3 <- "
UPDATE traps.trap_names
SET liquid_name = 'EtOH85'
WHERE trap_short_name LIKE 'MF%'
"
dbExecute(con, fill_extra_trap_col3)

fill_extra_trap_col4 <- "
UPDATE traps.trap_names
SET liquid_name = 'PropGl70_EtOH85'
WHERE trap_short_name LIKE 'VF%'
"
dbExecute(con, fill_extra_trap_col4)


```


  
```{r}

create_sampling_trap_event_table <- "
CREATE TABLE events.sampling_trap (
id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
genlab_GUID UUID,
sample_id text,
location_sampling_id UUID NOT NULL,
trap_id UUID,
comment_start text,
comment_end text,
gross_weight double precision,
weight_empty_bottle double precision,
wet_weight_bottle double precision,
wet_weight double precision,
dry_weight_bottle double precision,
dry_weight double precision,
added_number_melormer double precision,
added_number_c_maculatus double precision,
added_number_c_analis double precision,
added_number_c_chinensis double precision,
CONSTRAINT location_sampling_fk 
FOREIGN KEY (location_sampling_id) REFERENCES events.location_sampling(id)
ON DELETE RESTRICT
ON UPDATE CASCADE,
CONSTRAINT trap_fk 
FOREIGN KEY (trap_id) REFERENCES traps.trap_names(id)
ON DELETE RESTRICT
ON UPDATE CASCADE
)
"
dbSendStatement(con, "DROP TABLE IF EXISTS events.sampling_trap CASCADE")
dbSendStatement(con,
                create_sampling_trap_event_table)

```

4. Identification events.
  - Include information about the identification process
  - Could have multiple runs of the metabarcoding, or multiple identification methodologies for the same sampling trap event (manual vs. metabarcoding)
  - Could include identification effort (total read abundance, scope of manual identification)


```{r}

create_id_techniques_table <- "
CREATE TABLE lookup.identification_techniques (
id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
identification_name text UNIQUE,
identification_type text NOT NULL,
identification_details text
)
"

dbSendStatement(con, "DROP TABLE IF EXISTS lookup.identification_techniques")
dbSendStatement(con, create_id_techniques_table)

```


```{r}

create_identification_event <- "
CREATE TABLE events.identifications (
id UUID PRIMARY KEY DEFAULT gen_random_UUID(),
metabarcoding_id text,
sampling_trap_id UUID,
identification_name text NOT NULL,
identification_comment text,
read_abundance bigint,
totalvolum_atl_prot_k_ml double precision,
ekstraksjonsdato date,
ekstraksjonskit text,
ekstraksjonskommentar text,
subsamplet_volum_ul double precision,
elueringsvolum_ul double precision,
od double precision,
x260_280 double precision,
x260_230 double precision,
run text,
lab_person text,
kommentar_genlab text,
CONSTRAINT sampling_trap_fk 
FOREIGN KEY (sampling_trap_id) 
REFERENCES events.sampling_trap(id)
ON DELETE RESTRICT
ON UPDATE CASCADE,
CONSTRAINT identification_name_fk 
FOREIGN KEY (identification_name) 
REFERENCES lookup.identification_techniques(identification_name)
ON DELETE RESTRICT
ON UPDATE CASCADE
)
"

dbExecute(con, "DROP TABLE IF EXISTS events.identifications")
dbExecute(con, create_identification_event)

```


Now we can create the occurrence table.


```{r}
create_occurrences_schema <- "
CREATE SCHEMA IF NOT EXISTS occurrences
AUTHORIZATION ag_pgsql_radardata_admin;
"

dbSendStatement(con, create_occurrences_schema)

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA occurrences
GRANT ALL ON TABLES TO ag_pgsql_radardata_admin;
")

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA occurrences
GRANT SELECT ON TABLES TO ag_pgsql_radardata_ro;
")

dbSendStatement(con, "
ALTER DEFAULT PRIVILEGES IN SCHEMA occurrences
GRANT INSERT, SELECT, UPDATE ON TABLES TO ag_pgsql_radardata_rw;
")
```


```{r}

create_occ_table <- "
CREATE TABLE occurrences.observations (
id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
idenfication_id UUID,
id_kingdom text,
id_phylum text,
id_class text,
id_order text,
id_family text,
id_genus text,
id_species text,
species_latin text,
taxonomic_level text,
no_reads double precision,
identification_conficence text,
possible_marker_issue boolean,
probable_database_insufficiency boolean,
native boolean,
no_species_in_norway integer,
no_norwegian_species_in_database integer,
pct_db_coverage double precision

)"

dbExecute(con, "DROP TABLE IF EXISTS occurrences.observations")
dbExecute(con, create_occ_table)
```


Normalize the data and populate the tables
===========

This is a bit ugly, and need some manual fixing in the table. A one-off for this year.

```{r}
insert_location_sampling <- "
INSERT INTO events.location_sampling  
SELECT gen_random_uuid() as id, emptied.year_location_id, emptied.start_date, emptied.end_date, 
started.ended_by as started_by, emptied.ended_by, emptied.comment
FROM 
(SELECT sm.location, l.id as year_location_id, date_placed as start_date, date_emptied as end_date,
person_emptied as ended_by, comment
FROM catch_data.sample_metadata sm LEFT JOIN
locations.locations l on 
(sm.location = l.location AND l.year = date_part('year', sm.date_placed)) 
GROUP BY l.id, sm.location, date_placed, date_emptied, person_emptied, comment
ORDER BY sm.location, date_placed, date_emptied) emptied 
LEFT JOIN
(SELECT l.id as year_location_id, date_emptied as end_date, 
person_emptied as ended_by, comment
FROM catch_data.sample_metadata sm LEFT JOIN
locations.locations l on 
(sm.location = l.location AND l.year = date_part('year', sm.date_placed)) 
GROUP BY l.id, sm.location, date_emptied, person_emptied, comment
) started
ON (emptied.start_date = started.end_date
	AND emptied.year_location_id = started.year_location_id)
GROUP BY emptied.location, emptied.year_location_id, emptied.start_date, started.ended_by, emptied.ended_by, emptied.end_date, emptied.comment

"
```

Populate sampling_trap
```{r}
pop_sampling_trap <- " 
INSERT INTO events.sampling_trap
SELECT DISTINCT ON(\"GUID\") 
gen_random_uuid() as id,
\"GUID\"::uuid as genlab_guid, 
sample_id,
foo.id as location_sampling_id,
traps.id as trap_id,
NULL AS comment_start,
NULL as comment_end,
\"Vekt_flaske_med_EtOH_og_insekter_.g.\" as gross_weight,
\"Vekt_tom_flaske_.g.\" as weight_empty_bottle,
\"Vekt_flaske_._insekter_vaat_.g.\" as wet_weight_bottle,
\"Vaatvekt_.g.\" as wet_weight,
\"Vekt_flaske_._insekter_toerr_.g.\" as dry_weight_bottle,
\"Toerrvekt_.g.\" as dry_weight,
\"Tilsatt_antall_Melormer\" as added_number_melormer,
\"Tilsatt_antall_C.maculatus\" as added_number_c_maculatus,
\"Tilsatt_antall_C.analis_SI_USA_08.06.20\" as added_number_c_analis,
\"Tilsatt_antall_C.chinensis_08.06.20\" as added_number_c_chinensis
FROM catch_data.sample_metadata sm LEFT JOIN
traps.trap_names traps
ON (sm.location = traps.location
AND sm.trap = traps.trap_short_name),
(SELECT ls.*, yl.location 
FROM events.location_sampling ls,
 events.year_location yl
 WHERE ls.year_location_id = yl.id) foo
WHERE sm.location = foo.location
AND sm.date_placed = foo.start_date
AND sm.date_emptied = foo.end_date
"
dbExecute(con, pop_sampling_trap)

```

Populate identification table

```{r}
pop_identification_table <- "
INSERT INTO events.identifications
SELECT distinct on("GenlabID")
gen_random_uuid() as id,
"GenlabID" as metabarcoding_id,
st.id as sampling_trap_id,
'metabarcoding_myseq' as identification_name,
kommentar_genlab as idenfication_comment,
NULL as read_abundance,
"Totalvolum_.mengde_ATL_._prot.K_mL." as totalvolum_atl_prot_k_ml,
"Ekstraksjonsdato" as ekstraksjonsdato,
"Ekstraksjonskit" as ekstraksjonskit,
"Ekstraksjonskommentar" as ekstraksjonskommentar,
"Subsamplet_volum_.ul." subsamplet_volum_ul,
"Elueringsvolum" as elueringsvolum_ul,
"OD" as od,
"X260_280" as x260_280,
"X260_230" as x260_230,
run,
"Labperson" as lab_person,
"kommentar_genlab" as kommentar_genlab
FROM catch_data.sample_metadata sm,
events.sampling_trap st
WHERE sm."GUID"::uuid = st.genlab_guid
"
dbExecute(con, pop_identification_table)

```



